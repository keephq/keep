---
title: "vLLM Provider"
description: "The vLLM Provider enables integration with vLLM-deployed language models into Keep."
---
import AutoGeneratedSnippet from '/snippets/providers/vllm-snippet-autogenerated.mdx';

<Tip>
  The vLLM Provider supports querying language models deployed with vLLM for prompt-based interactions.
</Tip>

<AutoGeneratedSnippet />

## Connecting with the Provider

To connect to a vLLM deployment:

1. Deploy your vLLM instance or obtain the API endpoint of an existing deployment
2. Configure the API URL in your provider configuration
3. If your deployment requires authentication, configure the API key
