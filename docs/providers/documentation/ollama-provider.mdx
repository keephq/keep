---
title: "Ollama Provider"
description: "The Ollama Provider allows for integrating locally running Ollama language models into Keep."
---
import AutoGeneratedSnippet from '/snippets/providers/ollama-snippet-autogenerated.mdx';

<Tip>
  The Ollama Provider supports querying local Ollama models for prompt-based
  interactions. Make sure you have Ollama installed and running locally with your desired models.
</Tip>

### **Cloud Limitation**
This provider is disabled for cloud environments and can only be used in local or self-hosted environments.

<AutoGeneratedSnippet />

## Connecting with the Provider

To use the Ollama Provider:

1. Install Ollama on your system from [Ollama's website](https://ollama.ai).
2. Start the Ollama service.
3. Pull your desired model(s) using `ollama pull model-name`.
4. Configure the host URL in your Keep configuration.

## Prerequisites

- Ollama must be installed and running on your system.
- The desired models must be pulled and available in your Ollama installation.
- The Ollama API must be accessible from the host where Keep is running.