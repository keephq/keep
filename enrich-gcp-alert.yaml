id: enrich-gcp-alert
description: Enriched GCP Alert
disabled: false
triggers:
- type: manual
- filters:
  - key: source
    value: gcpmonitoring
  type: alert
consts: {}
name: Enriching GCP Alerts with logs
owners: []
services: []
steps:
- name: gcpmonitoring-step
  provider:
    config: '{{ providers.gcpmonitoring }}'
    type: gcpmonitoring
    with:
      as_json: false
      filter: "resource.type = \"cloud_run_revision\" \n{{alert.traceId}}\n-logName=\"projects/keephq-sandbox/logs/monitoring.googleapis.com%2FViolationOpenEventv1\"\n-jsonPayload.module=\"step\"\n-jsonPayload.module=\"workflow\"\n-jsonPayload.module=\"gcpmonitoring_provider\"\n"
      page_size: 1000
      project: '{{ alert.projectId }}'
      raw: false
      timedelta_in_days: 1
- name: tenant-id-step
  provider:
    config: '{{ providers.gcpmonitoring }}'
    type: gcpmonitoring
    with:
      as_json: false
      filter: "resource.type = \"cloud_run_revision\" \n{{alert.traceId}}\n-logName=\"projects/keephq-sandbox/logs/monitoring.googleapis.com%2FViolationOpenEventv1\"\n-jsonPayload.module=\"step\"\n-jsonPayload.module=\"workflow\"\n-jsonPayload.module=\"gcpmonitoring_provider\"\njsonPayload.tenant_id!=\"\"\n"
      page_size: 1000
      project: '{{ alert.projectId }}'
      raw: true
      timedelta_in_days: 1
- name: get-more-details
  provider:
    config: ' {{ providers.readonly }} '
    type: mysql
    with:
      as_dict: true
      enrich_alert:
      - key: customer_name
        value: results.name
      query: select * from tenant where id = '{{ steps.tenant-id-step.results.0.payload.tenant_id }}'
      single_row: true
- name: openai-step
  provider:
    config: '{{ providers.openai }}'
    type: openai
    with:
      prompt: 'Be conscience in your answer, do not make up things and do not answer questions that are not asked. You are a very talented engineer that receives context from GCP logs about an endpoint that returned 500 status code and reports back the root cause analysis. Here is the context: keep.json_dumps({{steps.gcpmonitoring-step.results}}) (it is a JSON list of log entries from GCP Logging). In your answer, also provide the message from the log entry that made you conclude the root cause and specify what your certainty level is that it is the root cause. (between 1-10, where 1 is low and 10 is high) Strip any special characters from the log entry and in general from your answer, no '' or " allowed. If any of the logs is a stacktrace with file and line number, point the user to the right line of code in https://www.github.com/keephq/keep'
- name: openai-step-building-blocks
  provider:
    config: '{{ providers.openai }}'
    type: openai
    with:
      prompt: 'Based on this output you created: {{steps.openai-step.results}} Create blocks of text that will be used in a Slack message You can read more about it here: https://api.slack.com/block-kit You should output a structured message in the format of a Slack block kit and nothing else. An example to your response should be this: ```- text: emoji: true text: New GCP Alert type: plain_text type: header - elements: - elements: - text: "Name: " type: text - style: bold: true text: "{{alert.name}}" type: text type: rich_text_section type: rich_text```'
actions:
- name: slack-action
  provider:
    config: '{{ providers.keephq }}'
    type: slack
    with:
      blocks:
      - text:
          emoji: true
          text: New GCP Alert
          type: plain_text
        type: header
      - elements:
        - elements:
          - text: 'Name: '
            type: text
          - style:
              bold: true
            text: '{{alert.name}}'
            type: text
          type: rich_text_section
        type: rich_text
      - text:
          emoji: true
          text: ' '
          type: plain_text
        type: section
      - elements:
        - elements:
          - text: 'Customer: '
            type: text
          - style:
              bold: true
            text: '{{alert.customer_name}}'
            type: text
          type: rich_text_section
        type: rich_text
      - text:
          emoji: true
          text: ' '
          type: plain_text
        type: section
      - elements:
        - elements:
          - text: 'Severity: '
            type: text
          - style:
              bold: true
            text: "{{alert.severity}} \U0001F6A8"
            type: text
          type: rich_text_section
        type: rich_text
      - text:
          emoji: true
          text: ' '
          type: plain_text
        type: section
      - elements:
        - elements:
          - text: 'Description: '
            type: text
          - style:
              bold: true
            text: '{{alert.description}}'
            type: text
          type: rich_text_section
        type: rich_text
      - type: divider
      - text:
          emoji: true
          text: Root cause analysis
          type: plain_text
        type: header
      - text:
          emoji: true
          text: '{{steps.openai-step.results}}'
          type: plain_text
        type: section
      - type: divider
      - elements:
        - action_id: actionId-0
          text:
            emoji: true
            text: Original Alert
            type: plain_text
          type: button
          url: '{{alert.url}}'
        - action_id: actionId-1
          text:
            emoji: true
            text: Logs
            type: plain_text
          type: button
          url: https://console.cloud.google.com/logs/query;query=resource.type%20%3D%20%22cloud_run_revision%22%0A{{alert.traceId}};duration=P1D?project=keephq-sandbox
        type: actions
      channel: C07UB0LNS3D
      enrich_alert:
      - key: slack_timestamp
        value: results.slack_timestamp
      message: '{{steps.openai-step.results}}'
- foreach: '{{steps.gcpmonitoring-step.results}}'
  if: '"{{ foreach.value.payload_exists }}" and "{{alert.slack_timestamp}}"'
  name: slack-action
  provider:
    config: '{{ providers.keephq }}'
    type: slack
    with:
      channel: C07UB0LNS3D
      message: keep.json_dumps({{ foreach.value.payload }})
      thread_timestamp: '{{alert.slack_timestamp}}'
